{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amex-Kaggle-Dask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchoubey/AMEX-Default-Prediction/blob/main/Amex_Kaggle_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's Begin**\n",
        "\n",
        "Mount Drive to connect to Data"
      ],
      "metadata": {
        "id": "2XAb90UnDj_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io8uNJi79onh",
        "outputId": "eae1e82b-9e16-4875-8959-fc6b6572518d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: DASK"
      ],
      "metadata": {
        "id": "_XOox2zxDYuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install dask[dataframe] --upgrade"
      ],
      "metadata": {
        "id": "1EmxPzeW-NqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63a4b53-3675-451f-8886-b858a13c3610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.21.6)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.12.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2022.1)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-2022.5.0 locket-1.0.0 partd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import time\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Read Data into a dask dataframe\n",
        "df = dd.read_csv('/content/drive/MyDrive/Data-Science/amex-default-prediction/train_data.csv')\n",
        "\n",
        "# Check Runtime of Dask Dataframe\n",
        "\n",
        "#start\n",
        "start = time.time()\n",
        "\n",
        "#execute\n",
        "metric = df.P_2.mean()\n",
        "print(f'{metric.compute()}')\n",
        "\n",
        "#end\n",
        "end = time.time()\n",
        "\n",
        "print(f'Time Taken: {end-start}')"
      ],
      "metadata": {
        "id": "JnK0pEsN90Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 2: PySpark"
      ],
      "metadata": {
        "id": "XTXcE1u4EQ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MQ_DMTHBjqFC",
        "outputId": "90abfde7-cbe3-4306-d321-d36a9622c804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Start a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "PYlI6W76G0v8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "45e90300-2955-471b-b9dc-370ad4c28c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 10.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=ce90aaf8ab52893d2ed247d2f885fc2a8919a5d718a8606f9fa6a6b2ce1670b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe65c351690>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://52f70987ec94:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "qEZqvhI0ybEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Rename column in PySpark](https://www.educba.com/pyspark-rename-column/)\n",
        "\n",
        "[Create new calculated column](https://www.datasciencemadesimple.com/extract-first-n-and-last-n-character-in-pyspark/)"
      ],
      "metadata": {
        "id": "1nBdtEya6sYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "df = spark.read.csv('/content/drive/MyDrive/Data-Science/amex-default-prediction/train_data.csv', header=True)\n",
        "\n",
        "# Get features (categorical & numerical)\n",
        "features = df.drop('customer_ID', 'S_2').columns\n",
        "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "num_cols = [col for col in features if col not in cat_cols]\n",
        "\n",
        "# Pre-process Time period\n",
        "df = df.withColumnRenamed('S_2', 'Month')\n",
        "df = df.withColumn('Month', concat(df.Month.substr(1,8), lit('01')))\n",
        "\n",
        "# Subset Data (Perform data operations for single column to speed up. Later, it will be scaled to all columns.)\n",
        "df_sample = df.select('customer_ID', 'Month', 'P_2')\n",
        "df_sample.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG_5Uc1bB9Dl",
        "outputId": "38413d43-c584-45f5-cc07-3079ccb48b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+------------------+\n",
            "|         customer_ID|     Month|               P_2|\n",
            "+--------------------+----------+------------------+\n",
            "|0000099d6bd597052...|2017-03-01|0.9384687191272548|\n",
            "+--------------------+----------+------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Calculate Aggregates of column in PySpark](https://stackoverflow.com/questions/53389938/pyspark-how-to-calculate-min-max-value-of-each-field-using-pyspark#:~:text=There%20are%20different%20functions%20you%20can%20use%20to,%28%22col_1%22%29%29%2C%20min%20%28col%20%28%22col_2%22%29%29%2C%20max%20%28col%20%28%22col_2%22%29%29%29.show%20%28%29)"
      ],
      "metadata": {
        "id": "u87RY9pj7KSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check #months available\n",
        "df_sample.agg(min(col(\"Month\")), max(col(\"Month\"))).show()\n",
        "\n",
        "# Check if #months is consistent for all customers\n",
        "cust_agg = df_sample.groupBy('customer_ID').agg(min(col(\"Month\")), max(col(\"Month\")))\n",
        "cust_agg.show(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_z_SjYohBKo",
        "outputId": "b119a84c-7e6c-4cbf-f833-ba25570d6c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|min(Month)|max(Month)|\n",
            "+----------+----------+\n",
            "|2017-03-01|2018-03-01|\n",
            "+----------+----------+\n",
            "\n",
            "+--------------------+----------+----------+\n",
            "|         customer_ID|min(Month)|max(Month)|\n",
            "+--------------------+----------+----------+\n",
            "|0000099d6bd597052...|2017-03-01|2018-03-01|\n",
            "|00000fd6641609c6e...|2017-03-01|2018-03-01|\n",
            "|00001b22f846c82c5...|2017-03-01|2018-03-01|\n",
            "|000041bdba6ecadd8...|2017-03-01|2018-03-01|\n",
            "|00007889e4fcd2614...|2017-03-01|2018-03-01|\n",
            "|000084e5023181993...|2017-03-01|2018-03-01|\n",
            "|000098081fde4fd64...|2017-03-01|2018-03-01|\n",
            "|0000d17a1447b25a0...|2017-03-01|2018-03-01|\n",
            "|0000f99513770170a...|2017-03-01|2018-03-01|\n",
            "|00013181a0c5fc8f1...|2017-03-01|2018-03-01|\n",
            "|0001337ded4e1c253...|2018-01-01|2018-03-01|\n",
            "|00013c6e1cec7c21b...|2017-03-01|2018-03-01|\n",
            "|0001812036f155833...|2017-03-01|2018-03-01|\n",
            "|00018dd4932409baf...|2017-03-01|2018-03-01|\n",
            "|000198b3dc70edd65...|2017-03-01|2018-03-01|\n",
            "|000201146e53cacdd...|2017-03-01|2018-03-01|\n",
            "|0002d381bdd8048d7...|2017-03-01|2018-03-01|\n",
            "|0002e335892f7998f...|2017-03-01|2018-03-01|\n",
            "|00031e8be98bc3411...|2017-03-01|2018-03-01|\n",
            "|000333075fb8ec6d5...|2017-03-01|2018-03-01|\n",
            "|000391f219520dbca...|2017-12-01|2018-03-01|\n",
            "|00039533fe0b61bcf...|2017-03-01|2018-03-01|\n",
            "|0003b7891c4978644...|2017-03-01|2018-03-01|\n",
            "|0003e58375faf9055...|2017-03-01|2018-03-01|\n",
            "|000445609ff2a39d2...|2017-03-01|2018-03-01|\n",
            "|000473eb907b57c8c...|2017-03-01|2018-03-01|\n",
            "|0004837f0c785928a...|2017-03-01|2018-03-01|\n",
            "|0004860c260168fca...|2017-07-01|2018-03-01|\n",
            "|0004b8596c4946866...|2017-03-01|2018-03-01|\n",
            "|0004e00358fc4dd63...|2017-03-01|2018-03-01|\n",
            "+--------------------+----------+----------+\n",
            "only showing top 30 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check null values\n",
        "# 2. Drop columns that have null records >50%\n",
        "# 3. For remaining, replace null records with 0"
      ],
      "metadata": {
        "id": "jI9SeR3j4z1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Link: Aggregating multiple columns on multiple criterias in PySpark](https://stackoverflow.com/questions/62620453/pyspark-groupby-and-aggregate-avg-and-first-on-multiple-columns)"
      ],
      "metadata": {
        "id": "ifEYkU7laeSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Aggregation for 18 months of data - first, last, mean\n",
        "funcs_list = [mean, first, last]\n",
        "cols_list = ['P_2']\n",
        "expr = [f(c).alias(str(f.__name__) + '_' + str(c)) for f in funcs_list for c in cols_list]\n",
        "df_sample_grouped = df_sample.groupBy('customer_ID').agg(*expr)\n",
        "df_sample_grouped.show(5)"
      ],
      "metadata": {
        "id": "5mu_cxHcElCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Lags using Window function in PySpark](https://www.educba.com/pyspark-lag/)"
      ],
      "metadata": {
        "id": "I3_RgWCI7ZEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "windowSpec = Window.partitionBy(\"customer_ID\").orderBy(\"Month\")\n",
        "df_sample = df_sample.withColumn(\"lag\", lag(\"P_2\",1).over(windowSpec))"
      ],
      "metadata": {
        "id": "aLcoGXz8ElKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.show(2)"
      ],
      "metadata": {
        "id": "ybKSDXNiElOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2621a2d-7e93-40a8-af05-e5144ebb56f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+------------------+------------------+\n",
            "|         customer_ID|     Month|               P_2|               lag|\n",
            "+--------------------+----------+------------------+------------------+\n",
            "|00000fd6641609c6e...|2017-03-01|0.9291219156224948|              null|\n",
            "|00000fd6641609c6e...|2017-04-01|0.9184305365007384|0.9291219156224948|\n",
            "+--------------------+----------+------------------+------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Next Steps -------------------------------------------------------------------\n",
        "\n",
        "# Null values: Drop columns with >50% records null\n",
        "# Null value treatment in columns with <50% records null\n",
        "\n",
        "# Months: 2017-03 to 2018-03 (13 months)\n",
        "# Not all customers have 13 months of data: what points to consider when evaluating aggregates?\n",
        "\n",
        "# Numerical columns: Mean, First, Last, Lag 1-4 were calculated. \n",
        "# Look into: Velocity, growth, acceleration, jerk, moving average. etc. \n",
        "\n",
        "# Categorical column treatment: \n",
        "# Roughly 10 columns are categorical (0/1). How to aggregate them. Ex: Count, Min, Max etc.\n",
        "\n",
        "# Execute skeleton code over entire training data \n",
        "# Google colab not good for long-run as it is interactive, any thoughts on which IDE to use?"
      ],
      "metadata": {
        "id": "TiPWKQrKElRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7I2KhnM6ElVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J0ig4gobElXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cfEAlABSElZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}